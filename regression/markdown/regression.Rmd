---
title: "Regression on small dataset"
author: "Michalina Cieslak and Maciej Switala"
date: "03/02/2020"
font-family: Times New Roman
output:
  html_document:
    latex_engine: xelatex
    mainfont: Times New Roman
    code_folding: hide
    theme: sandstone
    highlight: textmate
    df_print: paged
    toc: true 
    toc_depth: 4
    number_sections: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---
<style type="text/css">
body{
text-align: justify
}
h1.title {
  font-size: 28px;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
    font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 10px;
  font-family: "Times New Roman", Times, serif;
  text-align: center;
}
</style>
```{r setup, include=FALSE, fig.keep = 'last', warnings = FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning=FALSE)
#knitr::opts_chunk$set(cache = T)

#sets English language
Sys.setenv(LANG = "en")

#sets proper directory
setwd("C:\\Users\\Michalina\\Desktop\\Machine Learning\\nowe_projekt")

#installs all the needed packages
source("technicals_10.11.R")

#reads dataset, saves as data.frame
data <- read_csv("CarPrice_Assignment.csv")
data <- as.data.frame(data)

data[,"CarBrand"] <- word(data[,"CarName"],1)

#cleans data and recodes variables
source("data_preparation_11.12_small.R")
data_final <- data_preparation(data)

str(data_final)
data_final <- data_final[,-c(1,3)]

continuous_variables <- c(
  "wheelbase","carlength","carwidth",
  "carheight","curbweight",
  "enginesize","boreratio",
  "stroke","compressionratio",
  "horsepower","peakrpm","citympg","highwaympg", "price"
)

categorical_variables <- c(
  "fueltype","aspiration","doornumber","carbody","drivewheel","enginelocation",
  "enginetype","cylindernumber","fuelsystem","CarBrand", "symboling"
)

```
#Introduction
The project was prepared for Machine Learning 2 classes. It covers the topic of regression methods on small dataset. We perform different models: 

- linear regression (as a benchmark),
- stepwise regression,
- Ridge regression,
- Lasso regression,
- regrssion tree,
- kNN regression,

which will be compared based on the RMSE and R2 values.

# Overview of the problem 
Our dataset is retrived from: https://archive.ics.uci.edu/ml/datasets/Automobile. It referes to cars - describing their main features like, for example, model or number of doors. More precisely, it consists of 205 observations and 26 variables, which can be described as follows:

- car_ID - unique value of each observation
- symboling - assigned insurance risk rating, A value of +3 indicates that the car is risky, -3 that it is probably pretty safe.
- CarName - name of the car (including the model), i.e. audi 100ls
- fueltype - type of fuel, i.e. gas
- aspiration - aspiration used in a car
- doornumber - number of doors (two or four)
- carbody - frame of the car, i.e. hatchback
- drivewheel - type of drive, i.e. fwd (front-wheel drive)
- enginelocation - front or rear  
- wheelbase - distance between the centers of the front and rear wheels (in inches)
- carlength - length of a car (in inches)
- carwidth - width of a car (in inches)
- carheight - height of a car (in inches)
- curbweight - the total mass of a vehicle with standard equipment and all necessary operating consumable (in pounds)
- enginetype - type of valvetrain, i.e. ohc
- cylindernumber - cylinder placed in a car
- enginesize - size of a car 
- fuelsystem - type of fuel system, i.e. mpfi
- boreratio - boreratio of a car
- stroke - length of stroke (in inches)
- compressionratio - a ratio where you're compressing the maximum cylinder volume into the minimum cylinder volume
- horsepower
- peakrpm -  power band of an internal combustion engine or electric motor (in RPM)
- citympg - the lowest mpg rating
- highwaympg - the highest mpg rating
- price - price of the car 

**Our goal is to see which of the proposed models work best in predicting the value of a car given its' characteristics.**

#Data summary
The dataset has no missing values. Since the column CarName consists of many unique values, we created new variable (CarBrand), which takes only the first word from the former one and simply gives an information about the brand of a car. Some of the variables were encoded as characters, thus, we changed them to factors - in a way, that there is at most 15 levels. Furthermore, in the process of data preparation, we removed two columns, car_ID and CarName, because they consisted of unique, or almost only unique, values. Since the number of observations is rather small, we decided not to remove outliers. Instead, in further analysis, we used robust standarization, which is resistant to them. 

##Dependent variable
Price is our dependent variable. As it can be noticed on the graph and statistics below, it is highly and postively skewed. Mean price of all observations equals to 13277, while the highest one is 45400 and is assigned to a buick regal sport coupe (turbo).

```{r , echo= TRUE, message=FALSE, results='asis'}
cont <- data_final[,continuous_variables]
sum_pr <- round(basicStats(cont$price), 0)
sum_pr <- data.frame(t(sum_pr))
sum_pr <- sum_pr[,c(3,4,7,8,13,14,15,16)]
rownames(sum_pr) <- c('price')
kable(sum_pr) %>%
  kable_styling() 
```


```{r , echo= FALSE, message=FALSE, fig.align="center"}
d <- ggplot(data_final, aes(x=price)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#11CF1D") + 
  geom_vline(aes(xintercept=mean(price)),
             color="#1142CF", linetype="dashed", size=1) +
  labs(title='Density of price') +
  theme(legend.title = element_blank())
print(d)
```

##Independent variables

###Continuous independent variables
There are 13 continuous independent variables. All of them have, more or less, same distribution, the points are rather concentrated, except Compressionratio, where most of the obesrvations are smaller than 12 and the rest is higher than 20. 
```{r , echo= FALSE, fig.align="center"}
sum_cont <- round(basicStats(cont[,-14])[c("Mean", "Stdev", "Median", "Minimum", "Maximum"),], 0)
sum_cont <- data.frame(t(sum_cont))
kable(sum_cont) %>%
  kable_styling() 
```

```{r,echo=FALSE, message=FALSE, fig.align="center"}

##price vs other variables
##multiplot function from dr.Cwiakowski materials
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  plots <- c(list(...), plotlist)
  numPlots = length(plots)
  if (is.null(layout)) {
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  if (numPlots==1) {
    print(plots[[1]])
  } else {
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    for (i in 1:numPlots) {
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
p1 <- ggdraw() + draw_image("Rplot3.png")
p2 <- ggdraw() + draw_image("Rplot4.png")
p3 <- ggdraw() + draw_image("Rplot5.png")
p4 <- ggdraw() + draw_image("Rplot6.png")
p5 <- ggdraw() + draw_image("Rplot7.png")
p6 <- ggdraw() + draw_image("Rplot8.png")
p7 <- ggdraw() + draw_image("Rplot9.png")
p8 <- ggdraw() + draw_image("Rplot12.png")
p9 <- ggdraw() + draw_image("Rplot17.png")
p10 <- ggdraw() + draw_image("Rplot18.png")
p11 <- ggdraw() + draw_image("Rplot19.png")
p12 <- ggdraw() + draw_image("Rplot20.png")
p13 <- ggdraw() + draw_image("Rplot21.png")
p14 <- ggdraw() 

multiplot(p1, p2, p3, p4, cols=2)
multiplot(p5, p6, p7, p8, cols=2)
multiplot(p9, p10, p11, p12, cols=2)
multiplot(p13, p14, cols=2)
```

###Categorical independent variables
As it can be seen, majority of the categorical variables show disproportion between factors. In the case of some of them, it is logical and inevitable because the dataset is quite old (from 1987). Thus, it is obvious that e.g. most cars were riding on gas. Furtheromre, as greater number of cars are built in a specifict way, for instance usually the engine is placed in the front of the car (http://www.carkipedia.com/car-engine/engine-location.php), it is obvious there would occur such difference in frequencies. As mentioned before, some of the variables were encoded into levels, thus, i.e. CarBrand, which consisted of many different brand names, has a majority of observations that is put in the category called others.

```{r,echo=FALSE, message=FALSE, fig.align="center"}
p1 <- ggdraw() + draw_image("Rplot1.png")
p2 <- ggdraw() + draw_image("Rplot2.png")
p3 <- ggdraw() + draw_image("Rplot10.png")
p4 <- ggdraw() + draw_image("Rplot11.png")
p5 <- ggdraw() + draw_image("Rplot13.png")
p6 <- ggdraw() + draw_image("Rplot14.png")
p7 <- ggdraw() + draw_image("Rplot15.png")
p8 <- ggdraw() + draw_image("Rplot16.png")
p9 <- ggdraw() + draw_image("Rplot22.png")
p10 <- ggdraw() + draw_image("Rplot23.png")
p11 <- ggdraw() + draw_image("Rplot24.png")
p12 <- ggdraw() + draw_image("Rplot25.png")
p13 <- ggdraw() + draw_image("Rplot26.png")
p14 <- ggdraw() + draw_image("Rplot27.png")
p15 <- ggdraw() + draw_image("Rplot28.png")
p16 <- ggdraw() + draw_image("Rplot29.png")
p17 <- ggdraw() + draw_image("Rplot30.png")
p18 <- ggdraw() + draw_image("Rplot31.png")
p19 <- ggdraw() + draw_image("Rplot32.png") 
p20 <- ggdraw() + draw_image("Rplot33.png") 
p21 <- ggdraw() + draw_image("Rplot34.png") 
p22 <- ggdraw() + draw_image("Rplot35.png") 
#p23 <- ggdraw()
multiplot(p17, p3, p18, p4, cols=2)
multiplot(p5, p7, p6, p8, cols=2)
multiplot(p9, p11, p10, p12, cols=2)
multiplot(p13, p15, p14, p16, cols=2)
multiplot(p1, p19, p2, p20, cols=2)
multiplot(p22, p21, cols=2)
```

##Correlations 
The gaph below represents correlation plot of continuous variables. We used pearson metric. It can be noticed that nearly all of the variables are correlated above 50%. Only stroke, citympg and compressionratio are, more or less, not related to others. Such results are not suprising since a lot of car characteristics are interdependent, i.e. the bigger the size of the car the heavier it is.

```{r, message=FALSE, fig.width=12, fig.height=9, fig.keep = 'all', results='asis'}
#correlations' analyses
source('correlations_11.12.R')

#pearson for continuous variables
correlations(data_final,continuous_variables[,-14],1)
```
When it comes to categorical variables, beside carbody and doornumber, they show correlation at the level of 40% at maximum. The calculations were provided with the help of goodman-kruskal lambdas.

```{r, message=FALSE, fig.width=12, fig.height=9, fig.keep = 'all', results='asis'}
#goodman/kruskal lambda for categorical variables
correlations(data_final,categorical_variables,0)

```

#Data preparation 

```{r, message=FALSE, fig.width=15, fig.height=12, fig.keep = 'all', results='asis'}
data_final_enc <- as.data.frame(one_hot(as.data.table(data_final)))

data_final_enc_cont <- as.data.frame(one_hot(as.data.table(data_final[,continuous_variables])))

data_final_enc_cat <- as.data.frame(one_hot(as.data.table(data_final[,categorical_variables])))

#scaling dataset
data_final_enc_cont <- robustscale(data_final_enc_cont)

data_final_enc <- cbind(data_final_enc_cat, data_final_enc_cont$data)

data_final_enc <- data_final_enc %>% 
  rename(
    symboling_neg_1 = 'symboling_-1',
    symboling_neg_2 = 'symboling_-2'
  )

set.seed(12345)
##split into train and test
indice = sample(seq_len(nrow(data_final_enc)), size = floor(0.7*nrow(data_final_enc)))

train = data_final_enc[indice,]
test = data_final_enc[-indice,]

c_5 = trainControl(method = "cv", number = 5)
```

Because our dataset is small, we did not want to delete outliers and, instead, we decided to standarize data using robust scaler, which is resistant to extreme values (takes into account medians). In general, standardization will produce lots of negative numbers, because any value below the mean will be standardized to
be negative, which may make the interpretation of the results of the models harder. All of the categorical variables were encoded into dummies, which enlarged the dataset into 71 columns. We divided it randomly into 70% and 30% of train and test data respectively. As mentioned above, continuous variables are quite correlated, however, we decided to keep all of them and see how the models behave. In the case of regression they will probably influence the outcomes, however, we treat this model only as a benchmark. The other four should deal with this problem. Since our data is small we used 5 fold cross validation. To evaluate which model gives the best results, we checked Rsquared and RMSE metric. The former one tells us how well the model fits the data, while the other defines how far from the regression line are the predicted points. We decided to use RMSE over MAE, because it penalizes the large errors (https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d).

#Modelling
##Linear regression
```{r, fig.width=15, fig.height=12, fig.keep = 'all', results='hide', message=FALSE, echo=TRUE}

#function from the classes
source('regressionMetrics.R')

modelform <- price~.
set.seed(12345)
linear <- train(modelform, data = train,
                method = "lm",
                metric = "RMSE",
                trControl = c_5)

linear$results
summary(linear)

predictions_l <- predict(linear, test)

linear_m <- regressionMetrics(real=test$price, predicted=predictions_l)
linear_m

set.seed(12345)
model1 <- lm(price~., data=train)
step.model <- stepAIC(model1, direction='both', trace=TRUE)

modelformula <- as.formula(step.model$call)

set.seed(12345)
linear_2 <- train(modelformula, data = train,
                method = "lm",
                metric = "RMSE",
                trControl = c_5)

linear_2$results
summary(linear_2)

predictions_l_2 <- predict(linear_2, test)

linear_m_2 <- regressionMetrics(real=test$price, predicted=predictions_l_2)
linear_m_2
```
Firstly, we ran a linear regression including all of the variables. Since, many of them are insignificant and the results on test data are rather poor, we performed stepwise regression. It tries different combinations and then, basing on the value of AIC, returns best final model. Since we wanted to chose the formula most precisely, we set option direction to both (checked forward and backward selection).

According to stepAIC() function, formula for the model with the highest value of AIC is as follows:

**price ~ aspiration_std + carbody_convertible + carbody_sedan + 
    enginelocation_front + enginetype_dohc + enginetype_l + enginetype_ohc + 
    enginetype_ohcf + enginetype_ohcv + cylindernumber_eight + 
    cylindernumber_five + cylindernumber_four + cylindernumber_six + 
    fuelsystem_2bbl + fuelsystem_mpfi + CarBrand_bmw + CarBrand_dodge + 
    CarBrand_mitsubishi + CarBrand_plymouth + CarBrand_toyota + 
    CarBrand_volkswagen + symboling_neg_2 + carlength + carwidth + 
    curbweight + enginesize + boreratio + stroke + compressionratio + 
    peakrpm**

After running the regression with respect to above formula, it turned out that all the variables, except symboling_neg_2 and carbody_sedan, are significant with at most 10%.

###Results
The table below displays the results for both train and test sample for linear and stepwise regressions.

<center>
| **Model** | **Train Rsquared** | **Test Rsquared** | **Train RMSE** | **Test RMSE** | 
| --- | --- | --- | --- | --- |
|*linear regression* | `r round(linear$results$Rsquared*100,2)`% | `r round(linear_m$R2*100,2)`% | `r round(linear$results$RMSE,2)` | `r round(linear_m$RMSE,2)`|
|*stepwise regression* | `r round(linear_2$results$Rsquared*100,2)`% | `r round(linear_m_2$R2*100,2)`% | `r round(linear_2$results$RMSE,2)` | `r round(linear_m_2$RMSE,2)`|

</center>
According to the results, stepwise regression slightly improved the performance on the test data. However, the model still poorly predicts the price (given R2 equal to `r round(linear_m_2$R2*100,2)`%). For both regressions the difference between test and train R2 is very big, with the value of the former quite low and for the latter one high. This may be a result of overfitting. To deal with such a problem it might be worth trying to use Lasso or Ridge regression. When it comes to RMSE, the stepwise regression lowered it significantly, so that that the errors are smaller.

##Ridge and Lasso regressions
```{r, fig.width=15, fig.height=12, fig.keep = 'all', results='hide', message=FALSE}
lambda <- 10^seq(-3, 3, length = 100)
set.seed(12345)
ridge <- train(
  price ~., data = train, method = "glmnet",
  trControl = c_5,
  tuneGrid = expand.grid(alpha = 0, lambda = lambda), 
  standardize = FALSE
)

ridge$bestTune$lambda

predictions_r_tr <- ridge %>% predict(train) #, s=ridge$bestTune$lambda)
ridge_m_tr <- regressionMetrics(real=train$price, predicted=predictions_r_tr)
ridge_m_tr

predictions_r <- ridge %>% predict(test) #, s=ridge$bestTune$lambda)
ridge_m_tst <- regressionMetrics(real=test$price, predicted=predictions_r)
ridge_m_tst
####################
set.seed(12345)
lasso <- train(
  price ~., data = train, method = "glmnet",
  trControl = c_5,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda),
  standardize = FALSE
)

lasso$bestTune$lambda

predictions_ls_tr <- lasso %>% predict(train)#, s=lasso$bestTune$lambda)
lasso_m_tr <- regressionMetrics(real=train$price, predicted=predictions_ls_tr)
lasso_m_tr

predictions_ls <- lasso %>% predict(test)#, s=lassso$bestTune$lambda)
lasso_m_tst <- regressionMetrics(real=test$price, predicted=predictions_ls)
lasso_m_tst

```
As mentioned before, we want to reduce the difference between statistics on test and train data, so that the model will explain higher percent of the variability of the response data around it's mean. Firstly we ran Ridge, then Lasso regression. In both cases the lambda, which is simply the penalty, equals to `r ridge$bestTune$lambda`.

###Results
The table below displays the results for both, train and test, samples for linear, Lasso and Ridge regressions.

<center>
| **Model** | **Train Rsquared** | **Test Rsquared** | **Train RMSE** | **Test RMSE** | 
| --- | --- | --- | --- | --- |
|*linear regression* | `r round(linear$results$Rsquared*100,2)`% | `r round(linear_m$R2*100,2)`% | `r round(linear$results$RMSE,2)` | `r round(linear_m$RMSE,2)`|
|*Ridge regression* | `r round(ridge_m_tr$R2*100,2)`% | `r round(ridge_m_tst$R2*100,2)`% | `r round(ridge_m_tr$RMSE,2)` | `r round(ridge_m_tst$RMSE,2)`|
|*Lasso regression* | `r round(lasso_m_tr$R2*100,2)`% | `r round(lasso_m_tst$R2*100,2)`% | `r round(lasso_m_tr$RMSE,2)` | `r round(lasso_m_tst$RMSE,2)`|

</center>

Thanks to Lasso and Ridge penalization, the performance on test data improved significantly - R2 grew by almost 30 percentage points. RMSE values lowered compared to linear regression, however, their value for train data became larger than for the test one. It maybe due to the fact that our dataset is very small or, because, it was randomly split and the mean price of the train is twice as high as of the test sample (train: `r mean(train$price)`, test: `r mean(test$price)`).

##Regression tree
```{r, fig.width=15, fig.height=12, fig.keep = 'all', results='hide', message=FALSE}
set.seed(12345)
cars.tree_2 <- tree(price ~ ., train, mindev=0.001)

cars.cv_2 <- cv.tree(cars.tree_2, K = 5)

best.size_2 <- cars.cv_2$size[which(cars.cv_2$dev==min(cars.cv_2$dev))]
best.size_2

set.seed(12345)
cars.prune_2 <- prune.tree(cars.tree_2, best = best.size_2[length(best.size_2)])

cars.pred_2 <- predict(cars.tree_2, newdata = test)

cars.pred_2_tr <- predict(cars.tree_2, newdata = train)
tree_m_2_tr <- regressionMetrics(real=train$price, predicted=cars.pred_2_tr)
tree_m_2_tr

tree_m_2_tst <- regressionMetrics(real=test$price, predicted=cars.pred_2)
tree_m_2_tst
```
Furthermore, we decided to see how well our data can be predicted using regression tree. The result of best size of a tree after prunning equals to `r best.size_2[length(best.size_2)]`. We tried different values of mindev, the one that resulted in best metrics (relatively high R2 and low RMSE) equals to 0.001 
    
```{r,echo=FALSE, message=FALSE, fig.align="center"}
p1 <- ggdraw() + draw_image("tree.png")
p2 <- ggdraw() + draw_image("Rplot123.png")
multiplot(p1, p2, cols=1)
```
The first plot depicts the tree we obtained after prunning. It is hardly interpretable because of the previously standarized data. However, we can say that there are four constructions used - enginesize, curbweight, carwidth and stroke.The number of terminal nodes equals to 6. Since we want to focus more on the accuracy of the model, the second graph shows the relationship between predicted and real prices. It can be observed that it is rather linear. All of the points are close to the line, meaning that the prediction correlates with the true value.

###Results
Regression tree, again, significantly improved the value of R2 compared to already performed methods. At the same time lowering RMSE for both - train and test data. 

<center>

| **Model** | **Train Rsquared** | **Test Rsquared** | **Train RMSE** | **Test RMSE** | 
| --- | --- | --- | --- | --- |
|*linear regression* | `r round(linear$results$Rsquared*100,2)`% | `r round(linear_m$R2*100,2)`% | `r round(linear$results$RMSE,2)` | `r round(linear_m$RMSE,2)`|
|*regression tree* | `r round(tree_m_2_tr$R2*100,2)`% | `r round(tree_m_2_tst$R2*100,2)`% | `r round(tree_m_2_tr$RMSE,2)` | `r round(tree_m_2_tst$RMSE,2)`|
</center>


##kNN regressions
```{r, fig.width=15, fig.height=12, fig.keep = 'all', results='hide', message=FALSE}
set.seed(12345)
knn1 <- 
  train(price~., 
        data = train, 
        method = "knn",
        trControl = c_5, 
        standarize = FALSE,
        metric="RMSE")

knn1

knn1$finalModel
knn1$finalModel$k
knn1$bestTune$k

#knnPredict <- predict(knn1,newdata = train, s=knn1$bestTune$k)

predictions_knn_tr <- knn1 %>% predict(train)#, s=knn1$bestTune$k)
knn_m_tr <- regressionMetrics(real=train$price, predicted=predictions_knn_tr)
knn_m_tr

predictions_knn <- knn1 %>% predict(test)#, s=knn1$bestTune$k)
knn_m_tst <- regressionMetrics(real=test$price, predicted=predictions_knn)
knn_m_tst

```
The last regression we wanted to use is kNN. It assesses the value of the new data point based on the average of distances to the closests training points. According to the graph below, the number of neighbours that results in the smallest RMSE (and MAE) equals to 5.

```{r, fig.width=6, fig.height=3, fig.keep = 'all', results='hide', align='center', message=FALSE}
plot(knn1)
```

###Results
When it comes to R2, this type of regression is better than the linear and stepwise ones. However, there is the same problem with RMSE, that arised for Ridge and Lasso regression - train RMSE is higher than test. 
<center>
| **Model** | **Train Rsquared** | **Test Rsquared** | **Train RMSE** | **Test RMSE** | 
| --- | --- | --- | --- | --- |
|*linear regression* | `r round(linear$results$Rsquared*100,2)`% | `r round(linear_m$R2*100,2)`% | `r round(linear$results$RMSE,2)` | `r round(linear_m$RMSE,2)`|
|*kNN regression* | `r round(knn_m_tr$R2*100,2)`% | `r round(knn_m_tst$R2*100,2)`% | `r round(knn_m_tr$RMSE,2)` | `r round(knn_m_tst$RMSE,2)`|
</center>

#Conclusions
To conclude, depending on the goal of our research - whether we want to choose best model basing on how well dependent variable is interpreted by independent ones (R2) or by how much the predicted points are far from the reality (RMSE), the outcomes are different. All of the models, except stepwise regression, resulted with rather satisfying Rsquared values for both, train and test, sample. Suprisingly, for some of the models, RMSE represents some inconsistency in what it's normally expected - test velue is higher than train one. The best among all, taking into account both R2 and RMSE, is regression tree. 

| **Model** | **Train Rsquared** | **Test Rsquared** | **Train RMSE** | **Test RMSE** | 
| --- | --- | --- | --- | --- |
|*linear regression* | `r round(linear$results$Rsquared*100,2)`% | `r round(linear_m$R2*100,2)`% | `r round(linear$results$RMSE,2)` | `r round(linear_m$RMSE,2)`|
|*stepwise regression* | `r round(linear_2$results$Rsquared*100,2)`% | `r round(linear_m_2$R2*100,2)`% | `r round(linear_2$results$RMSE,2)` | `r round(linear_m_2$RMSE,2)`|
|*Ridge regression* | `r round(ridge_m_tr$R2*100,2)`% | `r round(ridge_m_tst$R2*100,2)`% | `r round(ridge_m_tr$RMSE,2)` | `r round(ridge_m_tst$RMSE,2)`|
|*Lasso regression* | `r round(lasso_m_tr$R2*100,2)`% | `r round(lasso_m_tst$R2*100,2)`% | `r round(lasso_m_tr$RMSE,2)` | `r round(lasso_m_tst$RMSE,2)`|
|*regression tree* | `r round(tree_m_2_tr$R2*100,2)`% | `r round(tree_m_2_tst$R2*100,2)`% | `r round(tree_m_2_tr$RMSE,2)` | `r round(tree_m_2_tst$RMSE,2)`|
|*kNN regression* | `r round(knn_m_tr$R2*100,2)`% | `r round(knn_m_tst$R2*100,2)`% | `r round(knn_m_tr$RMSE,2)` | `r round(knn_m_tst$RMSE,2)`|


The recommendation for improving the models further would be to work on the number of observations. Since it is very low, it may easily influence the outcomes of the predictions i.e. differences in RMSE (worse RMSE  but better R2 of Ridge, Lasso and kNN than in the case of linear regression). One way to make it more accurate, would be to use bootstraping or resampling. It may be also worth trying to eliminate some of the correlations by i.e. creating new variable that describes two interdependent ones. Furthermore, as the dataset is split randomly on test and train sample, mean prices of price is very different for both them. Thus, making them similar may resolve the problem with the higher train than test RMSE. Linear and stepwiise regression would probably yield better results when all of the assuptions of the former one would be fullfield.   


